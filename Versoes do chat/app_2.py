"""
app_2.py

Interface Streamlit para o Chatbot Documental Inteligente.
Fornece uma aplicaÃ§Ã£o web onde o usuÃ¡rio pode:
  - Enviar perguntas relacionadas ao conteÃºdo indexado
  - Visualizar os trechos de documentos usados como contexto
  - Conferir o tempo de resposta e as fontes consultadas
"""

import os
import time
from typing import Tuple, List

import streamlit as st
from dotenv import load_dotenv

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# Carregamento de variÃ¡veis de ambiente
load_dotenv()

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# ConfiguraÃ§Ãµes globais
# NÃºmero padrÃ£o de trechos (chunks) a recuperar para cada pergunta
K_RESULTS = int(os.getenv("K_RESULTS", 3))

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# ImportaÃ§Ã£o de mÃ³dulos internos
# Aqui buscamos o cliente Chroma e a funÃ§Ã£o de geraÃ§Ã£o via Ollama
try:
    from store.chroma_store import collection
    from llm.llm import obter_resposta_llama
except ImportError as e:
    # Se algum mÃ³dulo nÃ£o existir, exibe erro crÃ­tico e interrompe a execuÃ§Ã£o
    st.error(f"Erro crÃ­tico: MÃ³dulos nÃ£o encontrados â€“ {str(e)}")
    st.stop()


# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# FunÃ§Ãµes auxiliares

def get_context(query: str, k: int = K_RESULTS) -> Tuple[str, List[str]]:
    """
    Recupera os k trechos mais relevantes para a pergunta e extrai as fontes.

    Args:
        query (str): Texto da pergunta do usuÃ¡rio.
        k (int): Quantidade de resultados relevantes a retornar.

    Returns:
        Tuple[str, List[str]]:
            - Contexto concatenado (string) com os textos dos documentos.
            - Lista de nomes de arquivos que serviram de fonte.
    """
    try:
        # Consulta o ChromaDB solicitando documentos e metadados
        result = collection.query(
            query_texts=[query],
            n_results=k,
            include=["documents", "metadatas"]
        )
        documentos = result["documents"][0]
        metadados  = result["metadatas"][0]

        # Extrai nomes de arquivo Ãºnicos a partir dos metadados
        fontes = list({m["source"] for m in metadados if "source" in m})

        # Concatena todos os trechos em um Ãºnico texto
        contexto = "\n\n".join(documentos)
        return contexto, fontes

    except Exception as e:
        # Em caso de falha na busca, exibe erro na UI e retorna vazio
        st.error(f"Erro na busca de contexto: {str(e)}")
        return "", []


# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# ConfiguraÃ§Ã£o da pÃ¡gina Streamlit
st.set_page_config(
    page_title="ğŸ§  Chatbot Documental Inteligente",
    page_icon="ğŸ¤–",
    layout="centered"
)

# CSS customizado para tema escuro e balÃµes de chat
st.markdown("""
<style>
    :root {
        --fundo: #181C14;
        --usuario: #3C3D37;
        --assistente: #697565;
        --destaque: #ECDFCC;
    }
    .stApp { background-color: var(--fundo); color: var(--destaque); }
    .conversa-container { max-width: 800px; margin: 0 auto; padding: 1rem; }
    .user-message {
        background: var(--usuario); color: var(--destaque);
        border-radius: 15px 15px 0 15px; padding: 1.2rem;
        margin: 1rem 0 1rem 30%; position: relative;
        box-shadow: 2px 2px 5px rgba(0,0,0,0.1);
    }
    .bot-message {
        background: var(--assistente); color: var(--destaque);
        border-radius: 15px 15px 15px 0; padding: 1.2rem;
        margin: 1rem 30% 1rem 0; position: relative;
        box-shadow: -2px 2px 5px rgba(0,0,0,0.1);
    }
    .user-message::after {
        content: ''; position: absolute; right: -10px; top: 15px;
        border-width: 10px 0 10px 10px; border-style: solid;
        border-color: transparent transparent transparent var(--usuario);
    }
    .bot-message::before {
        content: ''; position: absolute; left: -10px; top: 15px;
        border-width: 10px 10px 10px 0; border-style: solid;
        border-color: transparent var(--assistente) transparent transparent;
    }
    .detalhes-tecnicos { background: #2a2d27; padding: 1rem; border-radius: 8px; margin: 1rem 0; }
</style>
""", unsafe_allow_html=True)

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# InicializaÃ§Ã£o do estado de sessÃ£o para histÃ³rico de conversas
if "history" not in st.session_state:
    st.session_state.history = []  # Cada item: dict com 'pergunta', 'resposta', 'tempo', 'fontes', 'contexto'

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# TÃ­tulo e subtÃ­tulo da aplicaÃ§Ã£o
st.title("ğŸ§  Chatbot Documental Inteligente")
st.caption("FaÃ§a perguntas sobre seus documentos e obtenha respostas contextualizadas")

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# Ãrea de entrada de texto (chat_input) e processamento da pergunta
with st.container():
    user_input = st.chat_input("Digite sua pergunta...", key="input")

    if user_input:
        with st.spinner("ğŸ” Analisando documentos..."):
            start_time = time.time()

            try:
                # 1) Recupera contexto e fontes
                contexto, fontes = get_context(user_input)

                # 2) Gera resposta chamando o LLM local via Ollama
                resposta = obter_resposta_llama(user_input, contexto)

                # 3) Calcula tempo de processamento
                processing_time = time.time() - start_time

                # 4) Adiciona interaÃ§Ã£o ao histÃ³rico
                st.session_state.history.append({
                    "pergunta":  user_input,
                    "resposta":  resposta,
                    "tempo":     f"{processing_time:.2f}s",
                    "fontes":    fontes,
                    "contexto":  contexto
                })

            except Exception as e:
                # Caso ocorra erro, exibe mensagem e interrompe apenas esta interaÃ§Ã£o
                st.error(f"Erro ao processar: {str(e)}")

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# RenderizaÃ§Ã£o do histÃ³rico de conversas
for interacao in st.session_state.history:
    # Bloco de mensagem do usuÃ¡rio (alinhado Ã  direita)
    st.markdown(f"""
    <div class="user-message">
        <strong>ğŸ‘¤ VOCÃŠ</strong><br><br>
        {interacao['pergunta']}
    </div>
    """, unsafe_allow_html=True)

    # Bloco de resposta do assistente (alinhado Ã  esquerda)
    st.markdown(f"""
    <div class="bot-message">
        <strong>ğŸ¤– ASSISTENTE</strong><br><br>
        {interacao['resposta']}
    </div>
    """, unsafe_allow_html=True)

    # Ãrea expansÃ­vel com detalhes tÃ©cnicos (tempo e fontes)
    with st.expander("âš™ï¸ Detalhes da Resposta"):
        cols = st.columns([1, 3])
        with cols[0]:
            st.metric("â± Tempo", interacao['tempo'])
        with cols[1]:
            if interacao['fontes']:
                st.write("**ğŸ“š Fontes utilizadas:**")
                for fonte in interacao['fontes']:
                    st.caption(f"â€¢ {fonte}")

    # Ãrea expansÃ­vel com o contexto completo utilizado no prompt
    with st.expander("ğŸ” Contexto completo utilizado"):
        st.code(interacao['contexto'], language='markdown')

    st.divider()
