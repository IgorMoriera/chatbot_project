# ðŸ§  Chatbot Documental Inteligente

Um projeto em **Python 3.11** que implementa um chatbot capaz de responder perguntas com base em documentos locais (PDF, CSV e TXT). Utiliza:

- **LangChain** para chunking de texto
- **SentenceTransformers** (`allâ€‘MiniLMâ€‘L6â€‘v2`) para gerar embeddings
- **ChromaDB** como vector store leve e persistente
- **Ollama** + **Gemma3** local para geraÃ§Ã£o de respostas
- **Streamlit** para interface web interativa

---

## ðŸ” VisÃ£o Geral

1. **IngestÃ£o de documentos**
   - Leitura de arquivos em `data/` via loaders especializados
   - Chunking de texto para criar pedaÃ§os de tamanho controlado
   - IndexaÃ§Ã£o de chunks com metadados (fonte, nÃºmero de pÃ¡gina, ID) no ChromaDB

2. **RecuperaÃ§Ã£o de contexto**
   - Busca dos _k_ chunks mais relevantes para a pergunta do usuÃ¡rio
   - ExtraÃ§Ã£o de trechos e lista de fontes para montar o contexto

3. **GeraÃ§Ã£o de resposta**
   - Montagem de prompt com instruÃ§Ãµes e contexto
   - Envio ao modelo Gemma3 local via Ollama HTTP API
   - Recebimento e exibiÃ§Ã£o da resposta

4. **Interface Web**
   - Upload ou uso de documentos jÃ¡ indexados
   - Caixa de chat com histÃ³rico de perguntas e respostas
   - VisualizaÃ§Ã£o de trechos de contexto, tempo de resposta e fontes
   - Tema escuro customizado em CSS

---

## ðŸ“ Estrutura de Pastas

```
chatbot_project/
â”œâ”€â”€ .env                    # VariÃ¡veis de configuraÃ§Ã£o (API, paths, etc.)
â”œâ”€â”€ requirements.txt        # DependÃªncias do projeto
â”œâ”€â”€ README.md               # Este arquivo (agora como .txt)
â”œâ”€â”€ data/                   # PDFs, CSVs e TXTs a indexar
â”‚   â”œâ”€â”€ exemplo.pdf
â”‚   â”œâ”€â”€ exemplo.csv
â”‚   â””â”€â”€ exemplo.txt
â”œâ”€â”€ loaders/                # Leitores de diferentes formatos
â”‚   â”œâ”€â”€ pdf_loader.py
â”‚   â”œâ”€â”€ csv_loader.py
â”‚   â””â”€â”€ txt_loader.py
â”œâ”€â”€ retriever/              # Chunking de texto
â”‚   â””â”€â”€ retriever.py
â”œâ”€â”€ embeddings/             # GeraÃ§Ã£o de embeddings
â”‚   â””â”€â”€ embedder.py
â”œâ”€â”€ store/                  # AbstraÃ§Ã£o do ChromaDB
â”‚   â””â”€â”€ chroma_store.py
â”œâ”€â”€ llm/                    # IntegraÃ§Ã£o com Ollama/Gemma3
â”‚   â””â”€â”€ llm.py
â”œâ”€â”€ pipeline.py             # Script de ingestÃ£o completo
â””â”€â”€ app.py                  # Interface Streamlit
```

---

## âš™ï¸ InstalaÃ§Ã£o

### 1. PrÃ©â€‘requisitos

- **Python 3.11** (recomendado)
- **Ollama** instalado e modelo **Gemma3** baixado localmente
- Git

### 2. Clonar e criar ambiente

```bash
git clone https://github.com/seu-usuario/chatbot_documental_inteligente.git
cd chatbot_documental_inteligente
python3.11 -m venv .venv
source .venv/bin/activate    # Windows: .venv\Scripts\activate
```

### 3. Instalar dependÃªncias

```bash
pip install -r requirements.txt
```

### 4. Configurar variÃ¡veis de ambiente

Crie um arquivo `.env` na raiz com:

```dotenv
# URL e modelo do Ollama/Gemma3
OLLAMA_URL=http://localhost:11434/api/generate
OLLAMA_MODEL=gemma3:1b

# Quantos chunks recuperar por pergunta (opcional)
K_RESULTS=3

# Pasta de persistÃªncia do Chroma (opcional)
CHROMA_PERSIST_DIR=chroma_db
```

---

## ðŸš€ Como usar

### 1. IngestÃ£o de documentos

Rode o pipeline de ingestÃ£o para indexar todo o conteÃºdo de `data/`:

```bash
python pipeline.py
```

VocÃª verÃ¡:

```
ðŸ—‘ï¸  Limpando coleÃ§Ã£o Chroma anterior...
âœ… ColeÃ§Ã£o limpa.

ðŸ“‚  Carregado 5 documentos de 'exemplo.pdf'
âœ…  'exemplo.pdf': 20 chunks indexados
â€¦
ðŸ  IngestÃ£o concluÃ­da: 50 novos chunks de data
ðŸ“Š Total de chunks na coleÃ§Ã£o: 50
```

> Sempre faÃ§a isso **antes** de iniciar a interface.

### 2. Iniciar a interface Streamlit

```bash
streamlit run app.py
```

Abra no navegador: `http://localhost:8501`

- **Digite sua pergunta** na caixa de chat.
- Veja o **contexto recuperado**, o **tempo de resposta** e as **fontes** utilizadas.
- Expanda o **contexto completo** se quiser inspecionar o trecho exato enviado ao LLM.

---

## ðŸ§© Principais MÃ³dulos

| MÃ³dulo                      | Responsabilidade                                               |
|-----------------------------|----------------------------------------------------------------|
| `loaders/*.py`              | Carregam PDF, CSV e TXT, retornando `[{"text", "metadata"}]`   |
| `retriever/retriever.py`    | Divide textos em chunks com `RecursiveCharacterTextSplitter`   |
| `embeddings/embedder.py`    | Gera embeddings com `SentenceTransformer("allâ€‘MiniLMâ€‘L6â€‘v2")`   |
| `store/chroma_store.py`     | Configura ChromaDB, adiciona documentos e limpa coleÃ§Ã£o        |
| `llm/llm.py`                | Monta prompt e chama Ollama/Gemma3 via HTTP API                |
| `pipeline.py`               | Pipeline completo: limpeza, ingestÃ£o incremental e relatÃ³rio   |
| `app.py`                    | Interface Streamlit de chat com estilo dark-mode e balÃµes      |

---

## âš ï¸ Dicas de Ajuste e ResoluÃ§Ã£o de Problemas

- **VersÃ£o do Python:** use 3.11 ou superior para evitar erros de sintaxe `dict | None`.
- **Limpeza da coleÃ§Ã£o:** `pipeline.py` usa `limpar_colecao()` para garantir Ã­ndice limpo. Se der erro, delete a pasta `chroma_db/`.
- **Ajustes de chunking:** modifique `chunk_size`/`chunk_overlap` em `retriever/retriever.py`.
- **NÃºmero de resultados:** ajuste `K_RESULTS` no `.env` ou em `app.py`.
- **Timeout LLM:** no `llm/llm.py`, altere o `timeout` conforme necessidade.

---

## ðŸ¤ Contribuindo

1. Fork este repositÃ³rio.
2. Crie uma branch para sua feature: `git checkout -b feature/nova-coisa`.
3. FaÃ§a commits claros e pequenos.
4. Abra um Pull Request descrevendo seu objetivo.

---

## ðŸ“„ LicenÃ§a

MIT Â© [Seu Nome]
Sintaâ€‘se Ã  vontade para usar, modificar e distribuir este projeto livremente!

---

> Desenvolvido por **[Seu Nome]** â€” Data Scientist & Engenheiro de Dados
> Entre em contato: seu.email@exemplo.com
