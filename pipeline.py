"""
pipeline.py

M√≥dulo respons√°vel por orquestrar o fluxo completo de ingest√£o de documentos:
  1. Limpa a cole√ß√£o Chroma existente de forma segura.
  2. Varre a pasta de dados em busca de arquivos PDF, CSV e TXT.
  3. Carrega cada arquivo com o loader apropriado.
  4. Realiza chunking dos textos e gera IDs/metadados consistentes.
  5. Indexa os chunks no ChromaDB, evitando duplicatas.
  6. Apresenta um relat√≥rio final com o total de chunks na cole√ß√£o.
"""

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
# Bibliotecas
import os
from pathlib import Path
from dotenv import load_dotenv

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
# Carrega vari√°veis de ambiente do arquivo .env
load_dotenv()
data_dir = os.getenv("DATA_DIR")

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
# Importa√ß√£o de loaders para diferentes tipos de arquivo
from loaders.pdf_loader import load_pdf
from loaders.csv_loader import load_csv
from loaders.txt_loader import load_txt

# Importa√ß√£o do splitter de texto
from retriever.retriever import chunk_documents

# Importa√ß√£o de fun√ß√µes de ChromaDB
from store.chroma_store import collection, add_documents, limpar_colecao

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
# Limpeza inicial da cole√ß√£o Chroma
print("üóëÔ∏è  Limpando cole√ß√£o Chroma anterior...")
if limpar_colecao():
    print("‚úÖ Cole√ß√£o limpa.\n")
else:
    print("‚ö†Ô∏è Aten√ß√£o: N√£o foi poss√≠vel limpar a cole√ß√£o completamente!")
    print("           Verifique os logs e reinicie o Chroma se necess√°rio.\n")


def ingest_new_files(data_dir: str = data_dir) -> None:
    """
    Realiza a ingest√£o incremental de documentos na cole√ß√£o Chroma.

    Passos principais:
      1. Valida a exist√™ncia do diret√≥rio de dados.
      2. Itera sobre arquivos PDF, CSV e TXT, filtrando por extens√£o.
      3. Verifica se j√° existem chunks com metadata['source'] igual ao nome do arquivo,
         para evitar reprocessamento de documentos j√° indexados.
      4. Carrega o conte√∫do bruto usando o loader correspondente.
      5. Executa chunking do texto e gera IDs √∫nicos e metadados padronizados.
      6. Adiciona os chunks ao ChromaDB usando 'add_documents'.

    Args:
        data_dir (str): Caminho para a pasta contendo os arquivos de entrada.
    """
    base = Path(data_dir)

    # Verifica se o diret√≥rio existe e √© v√°lido
    if not base.exists() or not base.is_dir():
        print(f"‚ö†Ô∏è Diret√≥rio {data_dir!r} n√£o encontrado ou n√£o √© uma pasta.")
        return

    total_indexed = 0
    valid_suffixes = {".pdf", ".csv", ".txt"}

    # Processa cada arquivo na pasta
    for file_path in sorted(base.iterdir()):
        suffix = file_path.suffix.lower()
        if suffix not in valid_suffixes:
            # Ignora arquivos sem extens√£o suportada
            continue

        # ‚Äî Verifica√ß√£o de duplicatas ‚Äî
        try:
            existing = collection.get(
                where={"source": file_path.name},
                include=["metadatas"]
            )
            if existing and existing.get("metadatas"):
                print(f"‚è≠Ô∏è  Pulando '{file_path.name}' (j√° indexado)")
                continue
        except Exception as error:
            print(f"‚ö†Ô∏è  Erro na verifica√ß√£o de duplicatas: {str(error)}")
            continue

        # ‚Äî Carregamento de documentos brutos ‚Äî
        try:
            loader = {
                ".pdf": load_pdf,
                ".csv": load_csv,
                ".txt": load_txt
            }[suffix]
            raw_docs = loader(str(file_path))
            print(f"üìÇ  Carregado {len(raw_docs)} documentos de '{file_path.name}'")
        except Exception as error:
            print(f"‚ùå  Erro cr√≠tico ao ler '{file_path.name}': {str(error)}")
            continue

        # ‚Äî Chunking e padroniza√ß√£o de metadados ‚Äî
        try:
            chunks = chunk_documents(raw_docs)
            for idx, chunk in enumerate(chunks):
                # Substitui metadata por um dict consistente
                chunk["metadata"] = {
                    "source": file_path.name,
                    "page": chunk.get("metadata", {}).get("page", 0),
                    "chunk_id": f"{file_path.stem}_{idx:04d}"
                }
                chunk["id"] = f"{file_path.stem}_{idx:04d}"
        except Exception as error:
            print(f"‚ùå  Erro no chunking de '{file_path.name}': {str(error)}")
            continue

        # ‚Äî Indexa√ß√£o no ChromaDB ‚Äî
        try:
            add_documents(chunks)
            print(f"‚òëÔ∏è  '{file_path.name}': {len(chunks)} chunks indexados")
            total_indexed += len(chunks)
        except Exception as error:
            print(f"‚ùå  Falha na indexa√ß√£o de '{file_path.name}': {str(error)}")
            continue

    # Relat√≥rio final de ingest√£o
    print(f"\n‚úÖ  Ingest√£o conclu√≠da: {total_indexed} novos chunks de {data_dir}")


if __name__ == "__main__":
    # Executa todo o pipeline de ingest√£o
    ingest_new_files()

    # Exibe o total de chunks na cole√ß√£o ap√≥s ingest√£o
    try:
        total_chunks = collection.count()
        print(f"üìä Total de chunks na cole√ß√£o: {total_chunks}")
    except Exception as e:
        print(f"\n‚ö†Ô∏è  N√£o foi poss√≠vel obter o total de chunks: {str(e)}")
