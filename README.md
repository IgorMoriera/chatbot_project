# üß† Chatbot Documental  
![Python](https://img.shields.io/badge/python-3.11-blue) ![License](https://img.shields.io/badge/license-MIT-green)

<div align="center">
  <img src="https://github.com/IgorMoriera/chatbot_project/blob/master/logo.png" width="200" height="200" />
</div>


Um projeto em **Python 3.11** que implementa um chatbot capaz de responder perguntas com base em documentos locais (PDF, CSV e TXT). Este projeto utiliza tecnologias modernas para processar, indexar e gerar respostas a partir de documentos, oferecendo uma interface web (via Streamlit) e mobile (via Telegram) interativa.

### Tecnologias Utilizadas

- **LangChain**: para dividir textos em peda√ßos (chunking).
- **SentenceTransformers** (`all-MiniLM-L6-v2`): modelo para gerar embeddings de texto.
- **ChromaDB**: banco vetorial leve e persistente para armazenar embeddings.
- **Gemma3**: modelo de LLM local para gera√ß√£o de respostas.
- **Streamlit**: interface web amig√°vel e interativa.
- **Telegram**: interface mobile para interagir via chatbot.

---

## üîç Vis√£o Geral

## üîç Arquitetura do Chatbot Documental

<p align="center">
  <img src="https://github.com/IgorMoriera/chatbot_project/blob/master/Arquitetura%20Chatbot.png" width="600" alt="Arquitetura do Chatbot Documental"/>
</p>

O Chatbot Documental foi desenhado em quatro camadas distintas, que juntas garantem a ingest√£o, indexa√ß√£o, recupera√ß√£o de contexto e gera√ß√£o de respostas de forma eficiente e coerente.

A primeira camada √© respons√°vel pela **Ingest√£o e Chunking**. Aqui, o sistema varre a pasta `data/` identificando todos os arquivos nos formatos PDF, CSV e TXT. Cada documento √© ent√£o fragmentado em peda√ßos menores ‚Äî chamados *chunks* ‚Äî usando a biblioteca LangChain, de modo a manter cada fatia em um tamanho aproximado de 500 tokens. Essa etapa de divis√£o √© fundamental para que o modelo de embeddings consiga capturar detalhes sem√¢nticos de cada trecho sem ultrapassar os limites de contexto. Em seguida, cada chunk √© transformado em um vetor num√©rico de alta dimens√£o (por exemplo, `[-0.0101, -0.0101, 0.0101, ‚Ä¶]`), que ser√° a representa√ß√£o sem√¢ntica daquele peda√ßo de texto.

Na segunda camada, conhecida como **Armazenamento Vetorial**, utilizamos o ChromaDB como reposit√≥rio dos embeddings gerados. Al√©m de armazenar cada vetor, tamb√©m registramos metadados essenciais ‚Äî como o nome do arquivo de origem, o n√∫mero da p√°gina e a posi√ß√£o exata do chunk dentro do documento. Esses metadados permitem que, depois, possamos apresentar ao usu√°rio n√£o apenas a informa√ß√£o correta, mas tamb√©m sua refer√™ncia de onde ela foi extra√≠da. O ChromaDB, por sua vez, mant√©m √≠ndices otimizados para buscas de similaridade, garantindo alta velocidade e escalabilidade mesmo quando lidamos com grandes volumes de dados.

A terceira camada diz respeito √† **Recupera√ß√£o de Contexto** por meio de buscas sem√¢nticas. Quando um usu√°rio envia uma pergunta, n√≥s primeiro convertemos esse texto em um embedding, usando o mesmo modelo e dimens√£o dos vetores de documento. Esse vetor de consulta √© ent√£o usado para interrogar o ChromaDB em busca dos N chunks mais pr√≥ximos semanticamente. Ao obter esses trechos, montamos um prompt que combina a pergunta original com os extratos de texto selecionados ‚Äî cada um acompanhado de sua refer√™ncia de arquivo e p√°gina ‚Äî a fim de fornecer ao modelo de linguagem toda a informa√ß√£o contextual necess√°ria para gerar uma resposta precisa.

Por fim, na camada de **Gera√ß√£o de Resposta e Interfaces**, o prompt enriquecido vai para o modelo Gemma3, rodando localmente via Ollama. Esse LLM processa o contexto e devolve uma resposta detalhada e relevante. Para entregar essa resposta ao usu√°rio, oferecemos duas interfaces: uma aplica√ß√£o web em Streamlit, que exibe o chat em um navegador de forma interativa e visualmente agrad√°vel, e um bot no Telegram, que permite ao usu√°rio conversar diretamente pelo app de mensagens. Assim, unimos a robustez e precis√£o do back-end sem√¢ntico com a praticidade de interfaces conhecidas e acess√≠veis.

> **Fluxo resumido:**  
> 1. Ingest√£o e fragmenta√ß√£o dos documentos  
> 2. Gera√ß√£o e indexa√ß√£o de embeddings no ChromaDB  
> 3. Busca sem√¢ntica e montagem de contexto  
> 4. Gera√ß√£o de resposta pelo LLM e entrega via Streamlit ou Telegram  


---

## üìÅ Estrutura de Pastas

```
chatbot_project/
‚îú‚îÄ‚îÄ .env                    # Vari√°veis de configura√ß√£o (API, paths, token do Telegram, etc.)
‚îú‚îÄ‚îÄ requirements.txt        # Depend√™ncias do projeto
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ data/                   # PDFs, CSVs e TXTs a indexar
‚îÇ   ‚îú‚îÄ‚îÄ exemplo.pdf
‚îÇ   ‚îú‚îÄ‚îÄ exemplo.csv
‚îÇ   ‚îî‚îÄ‚îÄ exemplo.txt
‚îú‚îÄ‚îÄ loaders/                # Leitores de diferentes formatos
‚îÇ   ‚îú‚îÄ‚îÄ pdf_loader.py
‚îÇ   ‚îú‚îÄ‚îÄ csv_loader.py
‚îÇ   ‚îî‚îÄ‚îÄ txt_loader.py
‚îú‚îÄ‚îÄ retriever/              # Chunking de texto
‚îÇ   ‚îî‚îÄ‚îÄ retriever.py
‚îú‚îÄ‚îÄ embeddings/             # Gera√ß√£o de embeddings
‚îÇ   ‚îî‚îÄ‚îÄ embedder.py
‚îú‚îÄ‚îÄ store/                  # Abstra√ß√£o do ChromaDB
‚îÇ   ‚îî‚îÄ‚îÄ chroma_store.py
‚îú‚îÄ‚îÄ llm/                    # Integra√ß√£o com Gemma3
‚îÇ   ‚îî‚îÄ‚îÄ llm.py
‚îú‚îÄ‚îÄ pipeline.py             # Script de ingest√£o dos dados
‚îú‚îÄ‚îÄ app.py                  # Interface Streamlit
‚îî‚îÄ‚îÄ telegram_bot.py         # Integra√ß√£o com Telegram
```

---

## ‚öôÔ∏è Instala√ß√£o

### 1. Pr√©-requisitos

- **Ambiente virtual** utilizando o Anaconda.


- **Python 3.11** (recomendado para compatibilidade).
- **Ollama** instalado e modelo **Gemma3** baixado localmente.
- **Git** para clonar o reposit√≥rio.
- **Telegram**: Crie um bot no Telegram e obtenha o token.

### 2. Clonar e Configurar o Ambiente

```bash
git clone https://github.com/seu-usuario/chatbot_documental_inteligente.git
cd chatbot_documental_inteligente
conda create --name nome_do_ambiente python=3.11
conda activate nome_do_ambiente
```

### 3. Instalar Depend√™ncias

```bash
pip install -r requirements.txt
```

### 4. Configurar Vari√°veis de Ambiente

1. **Crie** um arquivo chamado `.env` na raiz do seu projeto.  
2. **Copie** e **cole** o conte√∫do abaixo dentro dele.  
3. **Substitua** cada valor placeholder pelo dado real do seu ambiente.  
4. **Salve** o arquivo.

```Vari√°veis de ambiente
# Diret√≥rio dos arquivos de entrada/sa√≠da
DATA_DIR=/caminho/para/seus/arquivos

# Token do Telegram Bot (obtido via @BotFather)
TELEGRAM_TOKEN=SEU_TOKEN_DO_TELEGRAM_AQUI

# Quantos ‚Äúchunks‚Äù recuperar por requisi√ß√£o (opcional; padr√£o: 3)
K_RESULTS=3

# Endpoint e modelo para gera√ß√£o via Ollama/Gemma3
OLLAMA_URL=http://localhost:11434/api/generate
OLLAMA_MODEL=gemma3:1b

# Nome do modelo de embeddings e chave da Hugging Face
MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2
HF_API_KEY=SUA_HUGGINGFACE_API_KEY_AQUI

# Pasta onde o Chroma vai persistir o banco vetorial
CHROMA_PERSIST_DIR=chroma_db
```

**Nota**: Substitua `*` pelo token do seu bot do Telegram.


## üöÄ Como Usar

### 1. Ingest√£o de Documentos

Execute o script de ingest√£o para indexar os documentos da pasta `data/`:

```bash
python pipeline.py
```

**Sa√≠da esperada:**

```
üóëÔ∏è  Limpando cole√ß√£o Chroma anterior...
‚úÖ Cole√ß√£o limpa.

üìÇ  Carregado 5 documentos de 'exemplo.pdf'
‚úÖ  'exemplo.pdf': 20 chunks indexados
‚Ä¶
üèÅ  Ingest√£o conclu√≠da: 50 novos chunks de data
üìä Total de chunks na cole√ß√£o: 50
```

> **Nota:** Execute este passo antes de iniciar as interfaces web ou Telegram.

### 2. Iniciar a Interface Streamlit

```bash
streamlit run app.py
```

Acesse `http://localhost:8501` no navegador e:

- Digite perguntas no chat referente aos documentos indexados.
- Veja respostas, contexto, fontes e tempo de processamento.

### 3. Interagir via Telegram

Ap√≥s configurar o token do Telegram no `.env`, execute o script do bot:

```bash
python telegram_bot.py
```

- No Telegram, inicie uma conversa com o seu bot.
- Envie perguntas sobre os documentos j√° indexados.
- Receba respostas diretamente no chat do Telegram.

### Exemplo de Uso

**Pergunta (Telegram ou Streamlit):** "O que diz o exemplo.pdf sobre sustentabilidade?"\
**Resposta:** "O exemplo.pdf menciona que a sustentabilidade envolve equilibrar recursos naturais e desenvolvimento econ√¥mico..."\
**Contexto:** Trecho do PDF com a fonte indicada.

---

## üß© Principais M√≥dulos

| M√≥dulo | Fun√ß√£o Principal |
| --- | --- |
| `loaders/*.py` | L√™ PDF, CSV e TXT, retornando texto e metadados. |
| `retriever/retriever.py` | Divide textos em chunks com `RecursiveCharacterTextSplitter`. |
| `embeddings/embedder.py` | Gera embeddings usando `SentenceTransformer`. |
| `store/chroma_store.py` | Gerencia o ChromaDB (indexa√ß√£o e limpeza). |
| `llm/llm.py` | Integra com Ollama/Gemma3 para gerar respostas. |
| `pipeline.py` | Executa a ingest√£o completa dos documentos. |
| `app.py` | Interface Streamlit com chat e visualiza√ß√£o de resultados. |
| `telegram_bot.py` | Integra√ß√£o com Telegram para intera√ß√µes via chat. |

---

## ‚ö†Ô∏è Dicas de Ajuste e Resolu√ß√£o de Problemas

- **Python 3.11+**: Essencial para evitar erros de sintaxe como `dict | None`.
- **Erro na cole√ß√£o Chroma**: Delete a pasta `chroma_db/` e reexecute `pipeline.py`.
- **Ajuste de chunks**: Modifique `chunk_size` e `chunk_overlap` em `retriever/retriever.py`.
- **Mais/menos contexto**: Altere `K_RESULTS` no `.env` ou `app.py`.
- **Timeout do LLM**: Ajuste o par√¢metro `timeout` em `llm/llm.py`.
- **Telegram**: Certifique-se de que o token est√° corretamente configurado no `.env`.

---

## ü§ù Contribuindo

Quer ajudar a melhorar o projeto? Siga esses passos:

1. Fa√ßa um **fork** do reposit√≥rio.
2. Crie uma branch: `git checkout -b feature/sua-ideia`.
3. Fa√ßa commits claros e envie um **Pull Request** com uma descri√ß√£o detalhada.

> **Dica**: Contribui√ß√µes para melhorar a interface web (Streamlit) ou mobile (Telegram) s√£o especialmente bem-vindas!

---

## üìÑ Licen√ßa

MIT ¬© Igor Moreira\
Use, modifique e distribua este projeto livremente!

---

> Desenvolvido por Igor Moreira ‚Äì Data Scientist & Engenheiro de Dados\
> Contato: igor.moreira@fat.uerj.br